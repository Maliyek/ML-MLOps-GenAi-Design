{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce21813",
   "metadata": {},
   "source": [
    "# Recurrent(Feedback) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "- for sequential data processing\n",
    "\n",
    "this is a sweet apple\n",
    "\n",
    "\n",
    "- nlp tasks\n",
    "- speech recognition\n",
    "- time-series prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1365e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "- memorize parts of the inputs and use them to make accurate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ba2d7c",
   "metadata": {},
   "source": [
    "## Feed Forward Network\n",
    "\n",
    "\n",
    "<img src='r1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd4dcb",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "<img src='r2.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0743c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "                 this is a red ....{apple}\n",
    "0 - a            [0\n",
    "1 - are           0\n",
    "2 - apple         0\n",
    "3                 0\n",
    ".                 0\n",
    "4100 - this       1       \n",
    ".                 0\n",
    "10000             0]\n",
    "\n",
    "encoding -> one-hot encoding\n",
    "\n",
    "word embeddings -> word2vec - GloVe\n",
    "\n",
    "\n",
    "steps in RNN\n",
    "\n",
    "1 - Input a dataset\n",
    "2 - complex computation - randomly initializing variables(weights and biases)\n",
    "\n",
    "y = f(x) = Wx + b\n",
    "\n",
    "3 - generate prediction\n",
    "4 - compare the prediction with expected values and it will give us an error\n",
    "5 - propogate the error back to the same path in the network and adjust the variables\n",
    "\n",
    "6 - repeat 1 - 5 until we have optimum variables\n",
    "\n",
    "7 - prediction is made on unseen data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ceab30",
   "metadata": {},
   "source": [
    "# Formula\n",
    "\n",
    "<img src='r3.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - holds the information about the precious words in the sequence\n",
    "h0 will initialy be zero\n",
    "\n",
    "f() -> activation function -> tanh or sigmoid\n",
    "\n",
    "2 - calculate the predicted word vector, we use softmax to produce vector (probability distribution)\n",
    "summing up to 1\n",
    "\n",
    "3 - cross entropy - loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305fa88",
   "metadata": {},
   "source": [
    "# problems with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - Exploding gradients\n",
    "- large weights updates\n",
    "\n",
    "2 - Vanishing Gradients\n",
    "- gradient will goes to zero\n",
    "\n",
    "\n",
    "cause\n",
    "1 - chain in multiplication\n",
    "2 - long sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c17bee",
   "metadata": {},
   "source": [
    "# RNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29dd6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - Bidirectional RNN  (BRNN)\n",
    "2 - Gated Recurrent Unit (GRU)\n",
    "3 - Long Short Term Memory(LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c4d29",
   "metadata": {},
   "source": [
    "# Implementation using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda46ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1074e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "047cf2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RNN\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out, _ = self.rnn(x)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59561b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "output_size = 10   # number of classes (0-9 digits)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff06751a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:13<00:00, 758806.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 181698.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2364234.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = MNIST(root='./data', train=False, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06dc252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data loaders\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d607b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss function, optimizer\n",
    "\n",
    "model = SimpleRNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60b8fbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        images, labels = images.view(-1, 28, 28).to(device), labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # loss calculation\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}'.format(epoch, num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9cd1d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 93.87%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.view(-1, 28, 28).to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Accuracy on the test set: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf726f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f8e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6694ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
